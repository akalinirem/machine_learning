{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../img/nlp.png\" width=\"600\" height=\"270\">  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding = latin1 --> data içinde latin alfabsi var\n",
    "# r --> read\n",
    "data = pd.read_csv(r\"../../data/gender_classifier.csv\",encoding = \"latin1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 tane serious veya data frame birleştirmek\n",
    "data = pd.concat([data.gender,data.description],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male</td>\n",
       "      <td>i sing my own rhythm.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>male</td>\n",
       "      <td>I'm the author of novels filled with family dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>male</td>\n",
       "      <td>louis whining and squealing and all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>male</td>\n",
       "      <td>Mobile guy.  49ers, Shazam, Google, Kleiner Pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>female</td>\n",
       "      <td>Ricky Wilson The Best FRONTMAN/Kaiser Chiefs T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20045</th>\n",
       "      <td>female</td>\n",
       "      <td>(rp)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20046</th>\n",
       "      <td>male</td>\n",
       "      <td>Whatever you like, it's not a problem at all. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20047</th>\n",
       "      <td>male</td>\n",
       "      <td>#TeamBarcelona ..You look lost so you should f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20048</th>\n",
       "      <td>female</td>\n",
       "      <td>Anti-statist; I homeschool my kids. Aspiring t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20049</th>\n",
       "      <td>female</td>\n",
       "      <td>Teamwork makes the dream work.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20050 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       gender                                        description\n",
       "0        male                              i sing my own rhythm.\n",
       "1        male  I'm the author of novels filled with family dr...\n",
       "2        male                louis whining and squealing and all\n",
       "3        male  Mobile guy.  49ers, Shazam, Google, Kleiner Pe...\n",
       "4      female  Ricky Wilson The Best FRONTMAN/Kaiser Chiefs T...\n",
       "...       ...                                                ...\n",
       "20045  female                                               (rp)\n",
       "20046    male  Whatever you like, it's not a problem at all. ...\n",
       "20047    male  #TeamBarcelona ..You look lost so you should f...\n",
       "20048  female  Anti-statist; I homeschool my kids. Aspiring t...\n",
       "20049  female                     Teamwork makes the dream work.\n",
       "\n",
       "[20050 rows x 2 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(axis = 0,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.gender = [ 1 if each == \"female\" else 0 for each in data.gender]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REGULAR EXPRESSION (RE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Textler clean edilecek."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ricky Wilson The Best FRONTMAN/Kaiser Chiefs The Best BAND Xxxx Thank you Kaiser Chiefs for an incredible year of gigs and memories to cherish always :) Xxxxxxx'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_description = data.description[4]\n",
    "first_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ricky Wilson The Best FRONTMANKaiser Chiefs The Best BAND Xxxx Thank you Kaiser Chiefs for an incredible year of gigs and memories to cherish always  Xxxxxxx'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#a' dan z'ye-A'dan Z'ye olmayanları boşlukla değiştir.\n",
    "description = re.sub(\"[^a-z A-Z]\",\"\",first_description)\n",
    "description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ricky wilson the best frontmankaiser chiefs the best band xxxx thank you kaiser chiefs for an incredible year of gigs and memories to cherish always  xxxxxxx'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tüm harfleri küçük harflere çevirir.\n",
    "description = description.lower()\n",
    "description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IRREVALANT WORDS (STOPWORDS)\n",
    "Gereksiz kelimeler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# natural labguage tool kit\n",
    "import nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/irem/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kelimeleri ayırır ve bir listenn içinde depolar.\n",
    "# description = description.split()\n",
    "\n",
    "# Split yerine tokenizer kullanılabilir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/irem/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.data.path.append(\"/home/irem/Desktop/machine_learning/machine_learning/data/nltk_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kelimeleri ayırır ve bir listenn içinde depolar.\n",
    "description = nltk.word_tokenize(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "stre = \"shouldn't ve guzel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"shouldn't\", 've', 'guzel']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Örneğin should not algılanamaz. \n",
    "stre.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['should', \"n't\", 've', 'guzel']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk ile kelimeler algılanır.\n",
    "nltk.word_tokenize(stre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gereksiz kelimeleri çıkar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set --> Tekrar eden kelimeleri set et.\n",
    "description = [ word  for word in description if not word in set(stopwords.words(\"english\"))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LEMMATIZATION\n",
    "\n",
    "Kelimlerin köklerini bulma.\n",
    "\n",
    "loved - love  gitmeyeceğim - git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk as nlp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/irem/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = nlp.WordNetLemmatizer()\n",
    "description = [ lemma.lemmatize(word) for word in description ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# description içindeki herbir kelimeyi boşlukla birleştir ardından tekrar text oluşur.\n",
    "description = \"\".join(description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yukarıda sadece 4. data için yapılan işlem aşağıda tüm data için yapılacak."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_list = []\n",
    "\n",
    "for description in data.description:\n",
    "\n",
    "    # [^a-z A-Z] aralığında olmayanları boşlukla değiştir.\n",
    "    description = re.sub(\"[^a-z A-Z]\",\"\",description)\n",
    "\n",
    "    # Tüm data küçük harf.\n",
    "    description = description.lower()\n",
    "\n",
    "    # Parçalar.\n",
    "    description = nltk.word_tokenize(description)\n",
    "\n",
    "\n",
    "    description = [ word  for word in description if not word in set(stopwords.words(\"english\"))]\n",
    "\n",
    "    # Kök bulma\n",
    "    lemma = nlp.WordNetLemmatizer()\n",
    "    description = [ lemma.lemmatize(word) for word in description ]\n",
    "\n",
    "    # Boşlukla birleştir tekrara bi araya getir.\n",
    "    description = \" \".join(description)\n",
    "    description_list.append(description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BAG OF WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../img/nlp_1.png\" width=\"800\" height=\"270\">  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data içinde bulunan 32000 kelimeden 500 tanesini seç.\n",
    "max_features = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_words= \"english\" --> İngilizce de gereksiz kelimeleri at.\n",
    "# lowercase= --> Burada kullanılarak da tüm harfler küçük yapılabilir.\n",
    "# tokken_pattern kullanılarak da gereksiz karakterler kaldırılabilir fakat yukarıda \"description = re.sub(\"[^a-z A-Z]\",\"\",description)\" kullanıldığında ekleme yapılmadı.\n",
    "count_vectorizer = CountVectorizer(max_features = max_features,stop_words= \"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en sik kullanilan 500 kelimeler: ['account' 'activist' 'addict' 'adult' 'adventure' 'advocate' 'aka' 'alum'\n",
      " 'amazing' 'america' 'american' 'angel' 'animal' 'anime' 'app' 'area'\n",
      " 'art' 'artist' 'ask' 'aspiring' 'author' 'award' 'away' 'awesome' 'baby'\n",
      " 'bad' 'band' 'based' 'beautiful' 'beauty' 'beer' 'believe' 'best'\n",
      " 'better' 'big' 'bio' 'bit' 'bitch' 'black' 'blog' 'blogger' 'blue' 'book'\n",
      " 'booking' 'born' 'bot' 'boy' 'brand' 'breaking' 'building' 'business'\n",
      " 'buy' 'car' 'care' 'cat' 'cause' 'ceo' 'change' 'channel' 'check'\n",
      " 'chicago' 'chief' 'child' 'christ' 'christian' 'city' 'class' 'club'\n",
      " 'coach' 'coffee' 'college' 'come' 'comic' 'coming' 'communication'\n",
      " 'community' 'company' 'computer' 'conservative' 'consultant' 'contact'\n",
      " 'content' 'continuous' 'control' 'cool' 'country' 'county' 'cover'\n",
      " 'crazy' 'create' 'creative' 'creator' 'culture' 'current' 'currently'\n",
      " 'dad' 'daily' 'dance' 'data' 'day' 'deal' 'dedicated' 'design' 'designer'\n",
      " 'developer' 'development' 'didnt' 'die' 'digital' 'direction' 'director'\n",
      " 'dj' 'dm' 'doesnt' 'dog' 'dont' 'dream' 'dreamer' 'editor' 'education'\n",
      " 'el' 'email' 'en' 'end' 'endorsement' 'engineer' 'english' 'enjoy'\n",
      " 'entertainment' 'enthusiast' 'entrepreneur' 'estate' 'event' 'everyday'\n",
      " 'experience' 'expert' 'eye' 'facebook' 'family' 'fan' 'fanatic' 'fashion'\n",
      " 'father' 'favorite' 'fc' 'feed' 'feel' 'feminist' 'film' 'financial'\n",
      " 'finding' 'fitness' 'follow' 'followed' 'follower' 'food' 'football'\n",
      " 'forever' 'founder' 'free' 'freelance' 'friend' 'fuck' 'fucking' 'fun'\n",
      " 'future' 'game' 'gamer' 'gaming' 'gay' 'geek' 'girl' 'global' 'god'\n",
      " 'going' 'gon' 'good' 'google' 'got' 'government' 'graduate' 'graphic'\n",
      " 'great' 'group' 'guy' 'hair' 'happiness' 'happy' 'hard' 'hate' 'head'\n",
      " 'health' 'heart' 'hell' 'hello' 'help' 'helping' 'hey' 'hi' 'high'\n",
      " 'history' 'hockey' 'home' 'hope' 'host' 'house' 'huge' 'human' 'husband'\n",
      " 'idea' 'ig' 'ill' 'im' 'improve' 'independent' 'industry' 'info'\n",
      " 'information' 'inspirational' 'insta' 'instagram' 'interested'\n",
      " 'international' 'internet' 'ive' 'jesus' 'job' 'join' 'journalist'\n",
      " 'junkie' 'justin' 'kid' 'kind' 'king' 'know' 'la' 'lady' 'latest' 'laugh'\n",
      " 'law' 'le' 'leader' 'leading' 'league' 'learn' 'learning' 'let' 'life'\n",
      " 'lifestyle' 'light' 'like' 'link' 'little' 'live' 'living' 'local'\n",
      " 'london' 'long' 'look' 'looking' 'lost' 'lot' 'love' 'lover' 'loving'\n",
      " 'magazine' 'major' 'make' 'maker' 'making' 'man' 'management' 'manager'\n",
      " 'map' 'marketing' 'married' 'master' 'matter' 'mean' 'medium' 'meet'\n",
      " 'member' 'met' 'mind' 'miss' 'mobile' 'model' 'mom' 'money' 'mother'\n",
      " 'movie' 'mum' 'music' 'musician' 'na' 'national' 'need' 'nerd' 'network'\n",
      " 'new' 'news' 'night' 'north' 'nsfw' 'obsessed' 'offer' 'official' 'oh'\n",
      " 'old' 'online' 'open' 'opinion' 'organization' 'owner' 'page' 'partner'\n",
      " 'party' 'passion' 'passionate' 'peace' 'people' 'perfect' 'person'\n",
      " 'personal' 'photo' 'photographer' 'photography' 'pic' 'picture' 'pizza'\n",
      " 'place' 'play' 'player' 'playing' 'pm' 'podcast' 'politics' 'pop'\n",
      " 'positive' 'post' 'power' 'pr' 'price' 'pro' 'probably' 'producer'\n",
      " 'product' 'prof' 'professional' 'program' 'project' 'promoting' 'proud'\n",
      " 'provide' 'providing' 'public' 'quality' 'que' 'queen' 'question' 'radio'\n",
      " 'random' 'read' 'reader' 'reading' 'real' 'really' 'reason' 'red'\n",
      " 'reporter' 'research' 'resource' 'retired' 'retweets' 'review' 'right'\n",
      " 'rip' 'rock' 'rp' 'rt' 'run' 'running' 'sale' 'say' 'sc' 'school'\n",
      " 'science' 'seattle' 'secret' 'self' 'senior' 'series' 'service' 'share'\n",
      " 'sheher' 'shit' 'shop' 'short' 'simple' 'singer' 'single' 'site' 'small'\n",
      " 'smile' 'snapchat' 'social' 'software' 'solution' 'son' 'soul' 'sound'\n",
      " 'source' 'south' 'space' 'speaker' 'special' 'specialist' 'sport' 'st'\n",
      " 'star' 'start' 'state' 'station' 'stay' 'stop' 'story' 'street' 'strong'\n",
      " 'student' 'stuff' 'style' 'subscribe' 'success' 'support' 'supporter'\n",
      " 'talk' 'tea' 'teacher' 'team' 'tech' 'technology' 'tell' 'th' 'thats'\n",
      " 'thing' 'think' 'thought' 'time' 'tip' 'today' 'training' 'travel' 'true'\n",
      " 'truth' 'try' 'trying' 'tv' 'tweet' 'tweeting' 'tweetmyjobs' 'twitter'\n",
      " 'uk' 'united' 'university' 'update' 'use' 'video' 'view' 'visit' 'voice'\n",
      " 'wan' 'want' 'war' 'watch' 'way' 'weather' 'web' 'website' 'welcome'\n",
      " 'west' 'whats' 'white' 'wife' 'wine' 'winner' 'woman' 'word' 'work'\n",
      " 'working' 'world' 'write' 'writer' 'writing' 'year' 'yo' 'york' 'young'\n",
      " 'youre' 'youtube' 'youtuber']\n"
     ]
    }
   ],
   "source": [
    "# sparce_matrix --> Yukarıda verilen img içindeki 1 ve 0'lara sparce_matrix denir.\n",
    "sparce_matrix = count_vectorizer.fit_transform(description_list).toarray()\n",
    "\n",
    "print(\"en sik kullanilan {} kelimeler: {}\".format(max_features, count_vectorizer.get_feature_names_out()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
